{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":881667,"sourceType":"datasetVersion","datasetId":470244},{"sourceId":7940299,"sourceType":"datasetVersion","datasetId":4668301}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **GMM Model**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom sklearn.mixture import GaussianMixture\n\n# List of leaders' names\nleaders = ['Nelson_Mandela', 'Benjamin_Netanyau', 'Magaret_Tarcher', 'Jens_Stoltenberg', 'Julia_Gillard']\n\n# Path to the dataset\ndata_dir = '/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches'\n\n# Initialize lists to store file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop over the directories and files\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        # Get the full file path\n        file_path = os.path.join(dirname, filename)\n        \n        # Check if the file belongs to one of the leaders' classes\n        speaker = os.path.basename(dirname)\n        if speaker in leaders:\n            file_paths.append(file_path)\n            labels.append(speaker)\n\n# Convert labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2)\n\n# MFCC feature extraction\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    energy = np.sum(audio**2)\n    return energy\n\n# Extract features for train and test sets\nX_train_features = np.array([extract_features(file_path) for file_path in X_train])\nX_test_features = np.array([extract_features(file_path) for file_path in X_test])\n\n# Reshape features for GMM\nX_train_flat = X_train_features.reshape(X_train_features.shape[0], -1)\nX_test_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n\n# Number of Gaussian components\nnum_components = len(leaders)  # Assuming one component per speaker\n\n# Initialize GMM\ngmm = GaussianMixture(n_components=num_components, covariance_type='diag', random_state=42)\n\n# Fit GMM to the flattened MFCC features\ngmm.fit(X_train_flat)\n\n# Predict speaker labels for test set\npredicted_labels = gmm.predict(X_test_flat)\n\n# Evaluate accuracy\naccuracy = np.mean(predicted_labels == y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:56:51.113949Z","iopub.execute_input":"2024-03-26T18:56:51.115277Z","iopub.status.idle":"2024-03-26T18:57:05.355616Z","shell.execute_reply.started":"2024-03-26T18:56:51.115215Z","shell.execute_reply":"2024-03-26T18:57:05.354306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom sklearn.mixture import GaussianMixture\n\n# List of leaders' names\nleaders = ['Nelson_Mandela', 'Benjamin_Netanyau', 'Magaret_Tarcher', 'Jens_Stoltenberg', 'Julia_Gillard']\n\n# Path to the dataset\ndata_dir = '/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches'\n\n# Initialize lists to store file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop over the directories and files\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        # Get the full file path\n        file_path = os.path.join(dirname, filename)\n        \n        # Check if the file belongs to one of the leaders' classes\n        speaker = os.path.basename(dirname)\n        if speaker in leaders:\n            file_paths.append(file_path)\n            labels.append(speaker)\n\n# Convert labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2)\n\n# MFCC feature extraction\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n    mfccs = mfccs.T\n    return mfccs\n    return mfcc\n\n# Extract features for train and test sets\nX_train_features = np.array([extract_features(file_path) for file_path in X_train])\nX_test_features = np.array([extract_features(file_path) for file_path in X_test])\n\n# Reshape features for GMM\nX_train_flat = X_train_features.reshape(X_train_features.shape[0], -1)\nX_test_flat = X_test_features.reshape(X_test_features.shape[0], -1)\n\n# Number of Gaussian components\nnum_components = len(leaders)  # Assuming one component per speaker\n\n# Initialize GMM\ngmm = GaussianMixture(n_components=num_components, covariance_type='diag', random_state=42)\n\n# Fit GMM to the flattened MFCC features\ngmm.fit(X_train_flat)\n\n# Predict speaker labels for test set\npredicted_labels = gmm.predict(X_test_flat)\n\n# Evaluate accuracy\naccuracy = np.mean(predicted_labels == y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T18:57:05.35905Z","iopub.execute_input":"2024-03-26T18:57:05.359863Z","iopub.status.idle":"2024-03-26T18:58:29.036582Z","shell.execute_reply.started":"2024-03-26T18:57:05.35981Z","shell.execute_reply":"2024-03-26T18:58:29.034752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.mixture import GaussianMixture\n\n# List of leaders' names\nleaders = ['Nelson_Mandela', 'Benjamin_Netanyau', 'Magaret_Tarcher', 'Jens_Stoltenberg', 'Julia_Gillard']\n\n# Path to the dataset\ndata_dir = '/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches'\n\n# Initialize lists to store file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop over the directories and files\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        # Get the full file path\n        file_path = os.path.join(dirname, filename)\n        \n        # Check if the file belongs to one of the leaders' classes\n        speaker = os.path.basename(dirname)\n        if speaker in leaders:\n            file_paths.append(file_path)\n            labels.append(speaker)\n\n# Convert labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n\n# Define a function to extract MFCC features\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n    return mfccs.T\n\n# Initialize a dictionary to store GMMs for each speaker\ngmm_models = {}\n\n# Train a separate GMM for each speaker\nfor leader in leaders:\n    # Extract features for files belonging to the current speaker\n    X_train_speaker = [extract_features(file_path) for file_path, label in zip(X_train, y_train) if label == leaders.index(leader)]\n    X_train_speaker_flat = np.vstack(X_train_speaker)\n    \n    # Initialize GMM for the current speaker\n    gmm = GaussianMixture(n_components=5, covariance_type='diag', random_state=42)\n    \n    # Fit GMM to the flattened MFCC features\n    gmm.fit(X_train_speaker_flat)\n    \n    # Store the trained GMM in the dictionary\n    gmm_models[leader] = gmm\n\n# Predict speaker labels for test set\npredicted_labels = []\nfor file_path in X_test:\n    # Extract features for the current test file\n    test_features = extract_features(file_path)\n    \n    # Initialize variables to keep track of the predicted speaker and the highest log likelihood\n    predicted_speaker = None\n    max_log_likelihood = -np.inf\n    \n    # Predict speaker label using the GMM for each speaker and select the one with the highest log likelihood\n    for leader, gmm in gmm_models.items():\n        log_likelihood = gmm.score(test_features)\n        if log_likelihood > max_log_likelihood:\n            max_log_likelihood = log_likelihood\n            predicted_speaker = leader\n    \n    # Append the predicted label to the list\n    predicted_labels.append(leaders.index(predicted_speaker))\n\n# Evaluate accuracy\naccuracy = np.mean(predicted_labels == y_test)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T03:46:52.314851Z","iopub.execute_input":"2024-04-09T03:46:52.315328Z","iopub.status.idle":"2024-04-09T03:50:40.231241Z","shell.execute_reply.started":"2024-04-09T03:46:52.315290Z","shell.execute_reply":"2024-04-09T03:50:40.229805Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Accuracy: 0.9820119920053297\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **RNN Model**","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:04:54.638981Z","iopub.execute_input":"2024-03-26T19:04:54.640028Z","iopub.status.idle":"2024-03-26T19:04:54.647413Z","shell.execute_reply.started":"2024-03-26T19:04:54.639973Z","shell.execute_reply":"2024-03-26T19:04:54.645884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combining files of each speaker for visualization","metadata":{}},{"cell_type":"code","source":"import librosa\nimport soundfile as sf\n\n# Path to the dataset\ndataset_path = '/kaggle/input/leaderspeeches/16000_pcm_speeches'\n\n# Output directory to save the combined files\noutput_dir = '/kaggle/working/combined_files'\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_dir, exist_ok=True)\n\n# List of speaker folders\nspeaker_folders = [\n    \"Benjamin_Netanyau\",\n    \"Jens_Stoltenberg\",\n    \"Julia_Gillard\",\n    \"Magaret_Tarcher\",\n    \"Nelson_Mandela\"\n]\n\n# Number of files to combine for each speaker\nnum_files_to_combine = 200\n\n# Iterate over each speaker's folder\nfor speaker_folder in speaker_folders:\n    speaker_folder_path = os.path.join(dataset_path, speaker_folder)\n\n    # List the first num_files_to_combine WAV files in the speaker's folder\n    wav_files = [f\"{i}.wav\" for i in range(num_files_to_combine)]\n\n    # Combine all WAV files into a single long file\n    combined_audio = []\n    for wav_file in wav_files:\n        wav_file_path = os.path.join(speaker_folder_path, wav_file)\n        audio, sr = librosa.load(wav_file_path, sr=None)\n        combined_audio.extend(audio)\n\n    # Save the combined audio file\n    output_file_path = os.path.join(output_dir, f\"{speaker_folder}_combined.wav\")\n    sf.write(output_file_path, combined_audio, sr)\n\nprint(\"Combination complete. Combined files saved in:\", output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:04:54.652244Z","iopub.execute_input":"2024-03-26T19:04:54.652704Z","iopub.status.idle":"2024-03-26T19:04:59.225205Z","shell.execute_reply.started":"2024-03-26T19:04:54.65266Z","shell.execute_reply":"2024-03-26T19:04:59.223902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import display, Audio\n\n# Function to play audio file\ndef play_audio(audio_path):\n    display(Audio(filename=audio_path))\n\n# Play a specific combined audio file\nspeaker_folder = \"Benjamin_Netanyau_combined\"\naudio_path = os.path.join(output_dir, f\"{speaker_folder}.wav\")\nprint(f\"Click the play button to listen: {audio_path}\")\nplay_audio(audio_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:04:59.226594Z","iopub.execute_input":"2024-03-26T19:04:59.227109Z","iopub.status.idle":"2024-03-26T19:04:59.370354Z","shell.execute_reply.started":"2024-03-26T19:04:59.227051Z","shell.execute_reply":"2024-03-26T19:04:59.367979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualization","metadata":{}},{"cell_type":"code","source":"import librosa.display\n\n# Function to plot the waveform, spectrogram, and MFCCs\ndef plot_audio_features(audio_path):\n    # Load audio file\n    y, sr = librosa.load(audio_path, sr=None)\n\n    # Extract speaker name from the file path\n    speaker_name = os.path.basename(audio_path).split('_')[0]\n\n    # Plot the waveform\n    plt.figure(figsize=(15, 10))\n    plt.subplot(3, 1, 1)\n    librosa.display.waveshow(y, sr=sr)\n    plt.title(f'Waveform - {speaker_name}')\n\n    # Plot the spectrogram\n    plt.subplot(3, 1, 2)\n    D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)\n    librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f'Spectrogram - {speaker_name}')\n\n    # Plot the MFCCs\n    plt.subplot(3, 1, 3)\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n    librosa.display.specshow(mfccs, x_axis='time')\n    plt.colorbar()\n    plt.title(f'MFCCs - {speaker_name}')\n\n    plt.tight_layout()\n    plt.show()\n\n# Paths to the combined audio files\naudio_paths = [\n    '/kaggle/working/combined_files/Benjamin_Netanyau_combined.wav',\n    '/kaggle/working/combined_files/Jens_Stoltenberg_combined.wav'\n]\n\n# Plot features for each audio file\nfor audio_path in audio_paths:\n    plot_audio_features(audio_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:04:59.372166Z","iopub.execute_input":"2024-03-26T19:04:59.372635Z","iopub.status.idle":"2024-03-26T19:05:13.424224Z","shell.execute_reply.started":"2024-03-26T19:04:59.372586Z","shell.execute_reply":"2024-03-26T19:05:13.422834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Feature Extraction","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\n\n# Set the parent directory for speaker folders\nparent_dir = \"/kaggle/input/leaderspeeches/16000_pcm_speeches\"\n\n# List of speaker folders\nspeaker_folders = [\n    \"Benjamin_Netanyau\",\n    \"Jens_Stoltenberg\",\n    \"Julia_Gillard\",\n    \"Magaret_Tarcher\",\n    \"Nelson_Mandela\"\n]\n\ndef extract_features(parent_dir, speaker_folders):\n    features = []\n    labels = []\n\n    for i, speaker_folder in enumerate(speaker_folders):\n        speaker_folder_path = os.path.join(parent_dir, speaker_folder)\n\n        for filename in os.listdir(speaker_folder_path):\n            if filename.endswith(\".wav\"):\n                file_path = os.path.join(speaker_folder_path, filename)\n                audio, sr = librosa.load(file_path, sr=None, duration=1)\n                mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n                \n                # Normalize MFCC features\n                mfccs = StandardScaler().fit_transform(mfccs)\n                \n                features.append(mfccs.T)\n                labels.append(i)\n\n    return np.array(features), np.array(labels)\n\n# Extract features and labels\nX, y = extract_features(parent_dir, speaker_folders)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:05:13.425732Z","iopub.execute_input":"2024-03-26T19:05:13.426106Z","iopub.status.idle":"2024-03-26T19:06:52.431527Z","shell.execute_reply.started":"2024-03-26T19:05:13.426074Z","shell.execute_reply":"2024-03-26T19:06:52.430102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"MFCC","metadata":{}},{"cell_type":"code","source":"for feature in X[:1]:\n    print(feature)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:06:52.435632Z","iopub.execute_input":"2024-03-26T19:06:52.436754Z","iopub.status.idle":"2024-03-26T19:06:52.452496Z","shell.execute_reply.started":"2024-03-26T19:06:52.436698Z","shell.execute_reply":"2024-03-26T19:06:52.451217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset- 80% for training, 10% for validation, 10% for testing","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode labels with explicit classes\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\nlabel_encoder.classes_ = np.array(speaker_folders)\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Print the shapes of training and validation data\nprint(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_val.shape)\nprint(\"Testing Data Shape:\", X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:06:52.454738Z","iopub.execute_input":"2024-03-26T19:06:52.455675Z","iopub.status.idle":"2024-03-26T19:06:52.494487Z","shell.execute_reply.started":"2024-03-26T19:06:52.455626Z","shell.execute_reply":"2024-03-26T19:06:52.493237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RNN","metadata":{}},{"cell_type":"code","source":"# Define the RNN model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(len(speaker_folders), activation='softmax')\n])\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Define the EarlyStopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n\n# Train the model with EarlyStopping\nhistory = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, callbacks=[early_stopping])\n\n# Check if EarlyStopping triggered\nif early_stopping.stopped_epoch > 0:\n    print(\"Early stopping triggered at epoch\", early_stopping.stopped_epoch + 1)\nelse:\n    print(\"Training completed without early stopping\")\n\n# Plot training vs validation loss\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:06:52.49652Z","iopub.execute_input":"2024-03-26T19:06:52.497504Z","iopub.status.idle":"2024-03-26T19:07:30.665762Z","shell.execute_reply.started":"2024-03-26T19:06:52.497449Z","shell.execute_reply":"2024-03-26T19:07:30.664152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluation","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Evaluate the model on the test set\ny_pred_probabilities = model.predict(X_test)\ny_pred = np.argmax(y_pred_probabilities, axis=1)\n\n# Decode labels back to original format\ny_test_decoded = label_encoder.inverse_transform(y_test)\ny_pred_decoded = label_encoder.inverse_transform(y_pred)\n\n# Create a confusion matrix\nconf_matrix = confusion_matrix(y_test_decoded, y_pred_decoded, labels=speaker_folders)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test_decoded, y_pred_decoded)\nprint(f\"Test Evaluation Accuracy: {accuracy}\")\n\n# Calculate F1 score\nf1 = f1_score(y_test_decoded, y_pred_decoded, labels=speaker_folders, average='weighted')\nprint(f\"Weighted F1 Score: {f1}\")\n\n# Plot the confusion matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=speaker_folders, yticklabels=speaker_folders)\n\n# Rotate x-axis labels by 45 degrees\nplt.xticks(rotation=45, ha=\"right\")\n\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:07:30.667316Z","iopub.execute_input":"2024-03-26T19:07:30.667646Z","iopub.status.idle":"2024-03-26T19:07:31.919182Z","shell.execute_reply.started":"2024-03-26T19:07:30.667618Z","shell.execute_reply":"2024-03-26T19:07:31.91794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CNN and MLP Model**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n\n# List of leaders' names\nleaders = ['Nelson_Mandela', 'Benjamin_Netanyau', 'Magaret_Tarcher', 'Jens_Stoltenberg', 'Julia_Gillard']\n\n# Path to the dataset\ndata_dir = '/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches'\n\n# Initialize lists to store file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop over the directories and files\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        # Get the full file path\n        file_path = os.path.join(dirname, filename)\n        \n        # Check if the file belongs to one of the leaders' classes\n        speaker = os.path.basename(dirname)\n        if speaker in leaders:\n            file_paths.append(file_path)\n            labels.append(speaker)\n\n# Convert labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n\n# MFCC feature extraction\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n    mfccs = mfccs.T\n    return mfccs\n\n# Extract features for train and test sets\nX_train_features = np.array([extract_features(file_path) for file_path in X_train])\nX_test_features = np.array([extract_features(file_path) for file_path in X_test])\n\n# Reshape the features for the CNN model\nX_train_features = X_train_features.reshape(X_train_features.shape[0], X_train_features.shape[1], X_train_features.shape[2], 1)\nX_test_features = X_test_features.reshape(X_test_features.shape[0], X_test_features.shape[1], X_test_features.shape[2], 1)\n\n# Define the CNN model\nmodel = Sequential()\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=X_train_features.shape[1:]))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(len(leaders), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_features, y_train, epochs=50, batch_size=32, validation_data=(X_test_features, y_test))\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test_features, y_test)\nprint(f'Test accuracy: {test_acc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:13:45.641528Z","iopub.execute_input":"2024-03-26T19:13:45.641953Z","iopub.status.idle":"2024-03-26T19:18:53.460569Z","shell.execute_reply.started":"2024-03-26T19:13:45.641915Z","shell.execute_reply":"2024-03-26T19:18:53.458955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\n\n# List of leaders' names\nleaders = ['Nelson_Mandela', 'Benjamin_Netanyau', 'Magaret_Tarcher', 'Jens_Stoltenberg', 'Julia_Gillard']\n\n# Path to the dataset\ndata_dir = '/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches'\n\n# Initialize lists to store file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop over the directories and files\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        # Get the full file path\n        file_path = os.path.join(dirname, filename)\n        \n        # Check if the file belongs to one of the leaders' classes\n        speaker = os.path.basename(dirname)\n        if speaker in leaders:\n            file_paths.append(file_path)\n            labels.append(speaker)\n\n# Convert labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n\n# MFCC feature extraction\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n    mfccs = mfccs.T\n    return mfccs\n\n# Extract features for train and test sets\nX_train_features = np.array([extract_features(file_path) for file_path in X_train])\nX_test_features = np.array([extract_features(file_path) for file_path in X_test])\n\n# Define the MLP model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X_train_features.shape[1], X_train_features.shape[2])))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(len(leaders), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_features, y_train, epochs=50, batch_size=32, validation_data=(X_test_features, y_test))\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test_features, y_test)\nprint(f'Test accuracy: {test_acc:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T19:19:23.325525Z","iopub.execute_input":"2024-03-26T19:19:23.327595Z","iopub.status.idle":"2024-03-26T19:21:37.321749Z","shell.execute_reply.started":"2024-03-26T19:19:23.327529Z","shell.execute_reply":"2024-03-26T19:21:37.32022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport librosa\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\n\n# List of leaders' names\nleaders = ['Nelson_Mandela', 'Benjamin_Netanyau', 'Magaret_Tarcher', 'Jens_Stoltenberg', 'Julia_Gillard']\n\n# Path to the dataset\ndata_dir = '/kaggle/input/speaker-recognition-dataset/16000_pcm_speeches'\n\n# Initialize lists to store file paths and labels\nfile_paths = []\nlabels = []\n\n# Loop over the directories and files\nfor dirname, _, filenames in os.walk(data_dir):\n    for filename in filenames:\n        # Get the full file path\n        file_path = os.path.join(dirname, filename)\n        \n        # Check if the file belongs to one of the leaders' classes\n        speaker = os.path.basename(dirname)\n        if speaker in leaders:\n            file_paths.append(file_path)\n            labels.append(speaker)\n\n# Convert labels to numerical values\nlabel_encoder = LabelEncoder()\nlabels = label_encoder.fit_transform(labels)\n\n# Split the data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(file_paths, labels, test_size=0.2, random_state=42)\n\n# MFCC feature extraction\ndef extract_features(file_path):\n    audio, sr = librosa.load(file_path, sr=None)\n    stft = np.abs(librosa.stft(audio))\n    return stft\n\n# Extract features for train and test sets\nX_train_features = np.array([extract_features(file_path) for file_path in X_train])\nX_test_features = np.array([extract_features(file_path) for file_path in X_test])\n\n# Define the MLP model\nmodel = Sequential()\nmodel.add(Dense(64, activation='relu', input_shape=(X_train_features.shape[1], X_train_features.shape[2])))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(len(leaders), activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(X_train_features, y_train, epochs=50, batch_size=32, validation_data=(X_test_features, y_test))\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test_features, y_test)\nprint(f'Test accuracy: {test_acc:.4f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}